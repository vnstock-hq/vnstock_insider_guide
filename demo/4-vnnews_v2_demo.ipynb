{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/vnstock-hq/vnstock_insider_guide/blob/main/demo/4-vnnews_v2_demo.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jq8qYnMm9CS"
      },
      "source": [
        "# I. Cài đặt thư viện"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTCMHxzpm9CU"
      },
      "outputs": [],
      "source": [
        "# Sao chép mã nguồn từ GitHub về Google Colab\n",
        "!git clone https://github.com/vnstock-hq/vnstock_insider_guide\n",
        "# Cài đặt gói phụ thuộc\n",
        "%cd \"/content/vnstock_insider_guide/assets\"\n",
        "!pip install vnii-0.0.7.tar.gz\n",
        "# Chuyển đến thư mục vừa clone\n",
        "%cd /content/vnstock_insider_guide/oneclick_installer\n",
        "# Trao quyền thực thi cho file\n",
        "!chmod +x linux_installer.run\n",
        "# Chạy trình cài đặt\n",
        "!./linux_installer.run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIvOKk62m9CV"
      },
      "source": [
        "**Lưu ý: Bạn cần khởi động lại phiên làm việc sau khi cài đặt trên Google Colab**\n",
        "\n",
        "![restar session](http://vnstocks.com/images/khoi-dong-lai-phien-lam-viec-colab.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY8o3Wza2bvg"
      },
      "source": [
        "# Minh họa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm3DXnqB2gKh"
      },
      "source": [
        "## Chương trình mẫu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqW2gHix2c6b"
      },
      "outputs": [],
      "source": [
        "# Chuyển thư mục làm việc về nơi bạn muốn lưu dữ liệu tin tức\n",
        "%cd /content\n",
        "!vnstock-news-crawl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-ts_pk221xl"
      },
      "source": [
        "## Cập nhật tin tức từ 1 trang cụ thể"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSrRC9Gc4ax1"
      },
      "source": [
        "### Cập nhật tin tóm tắt từ Datafeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFtAtWks4dpf",
        "outputId": "d617dbf8-02e3-4795-f6eb-d721572ff367"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 09:53:05,686 - Cache - INFO - Initialized memory cache system\n",
            "2025-04-12 09:53:05,686 - export_feed_info - INFO - Starting the feed info export process...\n",
            "2025-04-12 09:53:05,687 - export_feed_info - INFO - Processing site: vnexpress\n",
            "2025-04-12 09:53:05,687 - Crawler - INFO - Using predefined configuration for site: vnexpress\n",
            "2025-04-12 09:53:05,687 - vnstock_news.core.sitemap - INFO - Attempting to load sitemap directly from https://vnexpress.net/sitemap.xml\n",
            "2025-04-12 09:53:05,885 - vnstock_news.core.sitemap - INFO - Sitemap loaded successfully using direct parsing.\n",
            "2025-04-12 09:53:05,885 - export_feed_info - INFO - Fetching feed info from sources: ['https://vnexpress.net/rss/tin-moi-nhat.rss']\n",
            "2025-04-12 09:53:05,886 - BatchCrawler - INFO - Checking source: https://vnexpress.net/rss/tin-moi-nhat.rss\n",
            "2025-04-12 09:53:05,887 - BatchCrawler - INFO - Parsing RSS feed: https://vnexpress.net/rss/tin-moi-nhat.rss\n",
            "2025-04-12 09:53:05,887 - vnstock_news.core.rss - INFO - Fetching RSS feed: https://vnexpress.net/rss/tin-moi-nhat.rss\n",
            "2025-04-12 09:53:06,038 - export_feed_info - INFO - Saved feed info with 20 records to output/feed_info/vnexpress_feed_info.csv\n",
            "2025-04-12 09:53:06,039 - export_feed_info - INFO - Feed info export process completed!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from vnstock_news.core.batch import BatchCrawler\n",
        "from vnstock_news.config.sites import SITES_CONFIG\n",
        "from vnstock_news.utils.logger import setup_logger\n",
        "\n",
        "# Initialize logger\n",
        "logger = setup_logger(\"export_feed_info\", debug=True)\n",
        "\n",
        "# Directory to store the CSV files\n",
        "OUTPUT_DIR = \"output/feed_info\"\n",
        "\n",
        "def fetch_feed_info(site_name: str, top_n: int = None, within: str = None):\n",
        "    \"\"\"\n",
        "    Fetch sitemap or RSS feed data and save the brief information to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "        site_name (str): Name of the site (from SITES_CONFIG).\n",
        "        top_n (int): Limit the number of articles to fetch.\n",
        "        within (str): Time frame to filter articles (e.g., '1h', '1d', '7d').\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize BatchCrawler\n",
        "        logger.info(f\"Processing site: {site_name}\")\n",
        "        crawler = BatchCrawler(site_name=site_name)\n",
        "\n",
        "        # Determine the source (sitemap or RSS)\n",
        "        site_data = SITES_CONFIG.get(site_name)\n",
        "        sources = []\n",
        "        if site_data.get(\"rss\"):\n",
        "            sources = site_data[\"rss\"][\"urls\"]\n",
        "        elif site_data.get(\"sitemap_url\"):\n",
        "            sources = [site_data[\"sitemap_url\"]]\n",
        "\n",
        "        if not sources:\n",
        "            logger.error(f\"No sources found for {site_name}. Skipping...\")\n",
        "            return\n",
        "\n",
        "        # Fetch feed data (without crawling article details)\n",
        "        logger.info(f\"Fetching feed info from sources: {sources}\")\n",
        "        feed_df = crawler.prepare_feeder(sources)\n",
        "\n",
        "        # Filter the data if time range or top_n is provided\n",
        "        if not feed_df.empty:\n",
        "            filtered_df = crawler.filter_feeder(feed_df, top_n=top_n, within=within)\n",
        "        else:\n",
        "            logger.warning(f\"No data fetched for {site_name}\")\n",
        "            return\n",
        "\n",
        "        # Output directory setup\n",
        "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "        output_file = os.path.join(OUTPUT_DIR, f\"{site_name}_feed_info.csv\")\n",
        "\n",
        "        # Save to CSV\n",
        "        filtered_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
        "        logger.info(f\"Saved feed info with {len(filtered_df)} records to {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing site '{site_name}': {e}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to fetch and save feed information for all supported sites.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting the feed info export process...\")\n",
        "\n",
        "    fetch_feed_info(site_name='vnexpress', top_n=20, within=\"1d\")  # Update as needed\n",
        "\n",
        "    logger.info(\"Feed info export process completed!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BojP2qZB6zYe"
      },
      "source": [
        "### Cập nhật tin chi tiết từ trang cụ thể"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac6ayB9B2kfR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import logging\n",
        "from vnstock_news.core.batch import BatchCrawler\n",
        "from vnstock_news.config.sites import SITES_CONFIG\n",
        "from vnstock_news.utils.logger import setup_logger\n",
        "\n",
        "# Initialize logger - fix the level parameter\n",
        "# The setup_logger function likely expects a logging level constant, not a string\n",
        "logger = setup_logger(\"update_news\", debug=False)  # Bật/Tắt log thông báo cơ bản\n",
        "\n",
        "# Directory to store the CSV files\n",
        "OUTPUT_DIR = \"output/cafebiz\"\n",
        "\n",
        "def fetch_and_save_news(site_name: str, top_n: int = 10, within: str = None):\n",
        "    \"\"\"\n",
        "    Fetch news articles from the specified site and save them to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "        site_name (str): Name of the site (from SITES_CONFIG).\n",
        "        top_n (int): Number of articles to fetch.\n",
        "        within (str): Time frame to filter articles (e.g., '1h', '2d').\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize the BatchCrawler\n",
        "        logger.info(f\"Processing site: {site_name}\")\n",
        "        crawler = BatchCrawler(site_name=site_name)\n",
        "\n",
        "        # Determine the source (sitemap or RSS)\n",
        "        site_data = SITES_CONFIG.get(site_name)\n",
        "        sources = []\n",
        "        if site_data.get(\"rss\"):\n",
        "            sources = site_data[\"rss\"][\"urls\"]\n",
        "        elif site_data.get(\"sitemap_url\"):\n",
        "            sources = [site_data[\"sitemap_url\"]]\n",
        "\n",
        "        if not sources:\n",
        "            logger.error(f\"No sources found for {site_name}. Skipping...\")\n",
        "            return\n",
        "\n",
        "        # Fetch articles\n",
        "        logger.info(f\"Fetching articles from sources: {sources}\")\n",
        "        articles_df = crawler.fetch_articles(sources, top_n=top_n, within=within)\n",
        "\n",
        "        # Output directory setup\n",
        "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "        output_file = os.path.join(OUTPUT_DIR, f\"{site_name}_news.csv\")\n",
        "\n",
        "        # Save to CSV\n",
        "        articles_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
        "        logger.info(f\"Saved {len(articles_df)} articles to {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing site '{site_name}': {e}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to fetch and save news for all supported sites.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting the news update process...\")\n",
        "\n",
        "    # Thay thế tên trang báo cần truy xuất dữ liệu\n",
        "    fetch_and_save_news(site_name='cafebiz', top_n=100, within=\"1h\")\n",
        "\n",
        "    logger.info(\"News update process completed!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g96C0Bko6FiV"
      },
      "source": [
        "## Cập nhật tin tức từ nhiều trang cùng lúc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfIoNOVx6ZZO"
      },
      "source": [
        "### Cập nhật tóm tắt từ Datafeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0-zn7t36dAN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from vnstock_news.core.batch import BatchCrawler\n",
        "from vnstock_news.config.sites import SITES_CONFIG\n",
        "from vnstock_news.utils.logger import setup_logger\n",
        "\n",
        "# Initialize logger\n",
        "logger = setup_logger(\"export_feed_info\", level=\"INFO\")\n",
        "\n",
        "# Directory to store the CSV files\n",
        "OUTPUT_DIR = \"output/feed_info\"\n",
        "\n",
        "def fetch_feed_info(site_name: str, top_n: int = None, within: str = None):\n",
        "    \"\"\"\n",
        "    Fetch sitemap or RSS feed data and save the brief information to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "        site_name (str): Name of the site (from SITES_CONFIG).\n",
        "        top_n (int): Limit the number of articles to fetch.\n",
        "        within (str): Time frame to filter articles (e.g., '1h', '1d', '7d').\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize BatchCrawler\n",
        "        logger.info(f\"Processing site: {site_name}\")\n",
        "        crawler = BatchCrawler(site_name=site_name)\n",
        "\n",
        "        # Determine the source (sitemap or RSS)\n",
        "        site_data = SITES_CONFIG.get(site_name)\n",
        "        sources = []\n",
        "        if site_data.get(\"rss\"):\n",
        "            sources = site_data[\"rss\"][\"urls\"]\n",
        "        elif site_data.get(\"sitemap_url\"):\n",
        "            sources = [site_data[\"sitemap_url\"]]\n",
        "\n",
        "        if not sources:\n",
        "            logger.error(f\"No sources found for {site_name}. Skipping...\")\n",
        "            return\n",
        "\n",
        "        # Fetch feed data (without crawling article details)\n",
        "        logger.info(f\"Fetching feed info from sources: {sources}\")\n",
        "        feed_df = crawler.prepare_feeder(sources)\n",
        "\n",
        "        # Filter the data if time range or top_n is provided\n",
        "        if not feed_df.empty:\n",
        "            filtered_df = crawler.filter_feeder(feed_df, top_n=top_n, within=within)\n",
        "        else:\n",
        "            logger.warning(f\"No data fetched for {site_name}\")\n",
        "            return\n",
        "\n",
        "        # Output directory setup\n",
        "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "        output_file = os.path.join(OUTPUT_DIR, f\"{site_name}_feed_info.csv\")\n",
        "\n",
        "        # Save to CSV\n",
        "        filtered_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
        "        logger.info(f\"Saved feed info with {len(filtered_df)} records to {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing site '{site_name}': {e}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to fetch and save feed information for all supported sites.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting the feed info export process...\")\n",
        "\n",
        "    # Iterate through all supported sites\n",
        "    for site_name in SITES_CONFIG.keys():\n",
        "        fetch_feed_info(site_name=site_name, top_n=20, within=\"1d\")  # Update as needed\n",
        "\n",
        "    logger.info(\"Feed info export process completed!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heIudTZm6dh7"
      },
      "source": [
        "### Cập nhật tin chi tiết từ nhiều trang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdu1CDHA6h-O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import logging\n",
        "from vnstock_news.core.batch import BatchCrawler\n",
        "from vnstock_news.config.sites import SITES_CONFIG\n",
        "from vnstock_news.utils.logger import setup_logger\n",
        "\n",
        "# Initialize logger - fix the level parameter\n",
        "# The setup_logger function likely expects a logging level constant, not a string\n",
        "logger = setup_logger(\"update_news\", debug=False)  # Using debug=False for INFO level\n",
        "\n",
        "# If your setup_logger was modified to accept an integer level:\n",
        "# logger = setup_logger(\"update_news\", level=logging.INFO)\n",
        "\n",
        "# Directory to store the CSV files\n",
        "OUTPUT_DIR = \"output/news\"\n",
        "\n",
        "def fetch_and_save_news(site_name: str, top_n: int = 10, within: str = None):\n",
        "    \"\"\"\n",
        "    Fetch news articles from the specified site and save them to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "        site_name (str): Name of the site (from SITES_CONFIG).\n",
        "        top_n (int): Number of articles to fetch.\n",
        "        within (str): Time frame to filter articles (e.g., '1h', '2d').\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize the BatchCrawler\n",
        "        logger.info(f\"Processing site: {site_name}\")\n",
        "        crawler = BatchCrawler(site_name=site_name)\n",
        "\n",
        "        # Determine the source (sitemap or RSS)\n",
        "        site_data = SITES_CONFIG.get(site_name)\n",
        "        sources = []\n",
        "        if site_data.get(\"rss\"):\n",
        "            sources = site_data[\"rss\"][\"urls\"]\n",
        "        elif site_data.get(\"sitemap_url\"):\n",
        "            sources = [site_data[\"sitemap_url\"]]\n",
        "\n",
        "        if not sources:\n",
        "            logger.error(f\"No sources found for {site_name}. Skipping...\")\n",
        "            return\n",
        "\n",
        "        # Fetch articles\n",
        "        logger.info(f\"Fetching articles from sources: {sources}\")\n",
        "        articles_df = crawler.fetch_articles(sources, top_n=top_n, within=within)\n",
        "\n",
        "        # Output directory setup\n",
        "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "        output_file = os.path.join(OUTPUT_DIR, f\"{site_name}_news.csv\")\n",
        "\n",
        "        # Save to CSV\n",
        "        articles_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
        "        logger.info(f\"Saved {len(articles_df)} articles to {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing site '{site_name}': {e}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to fetch and save news for all supported sites.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting the news update process...\")\n",
        "\n",
        "    # Iterate through all supported sites\n",
        "    for site_name in SITES_CONFIG.keys():\n",
        "        fetch_and_save_news(site_name=site_name, top_n=10, within=\"1d\")  # Update as needed\n",
        "\n",
        "    logger.info(\"News update process completed!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "g2sUII4u6ETP"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
